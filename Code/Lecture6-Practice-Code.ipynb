{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use today is **Boston Dataset**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from sklearn import feature_selection\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.9</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.9</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim  zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "\n",
       "   black  lstat  medv  \n",
       "0  396.9   4.98  24.0  \n",
       "1  396.9   9.14  21.6  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/ga-students/SF-DAT-20/master/Data/Boston.csv\"\n",
    "BostonData = pd.read_csv(url)\n",
    "del BostonData['Unnamed: 0']\n",
    "BostonData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston data frame has 506 rows and 14 columns.\n",
    "Usage\n",
    "\n",
    "Boston\n",
    "\n",
    "Format\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "crim\n",
    "\n",
    "    per capita crime rate by town \n",
    "    \n",
    "zn\n",
    "\n",
    "    proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "    \n",
    "indus\n",
    "\n",
    "    proportion of non-retail business acres per town \n",
    "    \n",
    "chas\n",
    "\n",
    "    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "    \n",
    "nox\n",
    "\n",
    "    nitrogen oxides concentration (parts per 10 million) \n",
    "    \n",
    "rm\n",
    "\n",
    "    average number of rooms per dwelling \n",
    "    \n",
    "age\n",
    "\n",
    "    proportion of owner-occupied units built prior to 1940 \n",
    "    \n",
    "dis\n",
    "\n",
    "    weighted mean of distances to five Boston employment centres \n",
    "    \n",
    "rad\n",
    "\n",
    "    index of accessibility to radial highways \n",
    "    \n",
    "tax\n",
    "\n",
    "    full-value property-tax rate per 10,000 dollars\n",
    "    \n",
    "ptratio\n",
    "\n",
    "    pupil-teacher ratio by town \n",
    "    \n",
    "black\n",
    "\n",
    "    1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "    \n",
    "lstat\n",
    "\n",
    "    lower status of the population (percent) \n",
    "    \n",
    "medv\n",
    "\n",
    "    median value of owner-occupied homes in 1000 dollars\n",
    "\n",
    "Source\n",
    "\n",
    "Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.\n",
    "\n",
    "Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n",
    "[Package MASS version 7.2-29 Index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal is to predict the median value of properties (medv) based on other variables in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's draw a scatter-plot of medv and lstat. Intuitively, does it like a pure linear association or it seems like there is some sort of non-linearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x113382b50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZrCwMwzDxIz/D1UKTBNB5x/OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112fbd210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(BostonData.medv,BostonData.lstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It does not look strictly linear.  It looks like there is quadratic or something higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's first define few non-linear terms. Start from a pure linear function and go up to polynomial degree 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BostonData['lstat_2'] = BostonData['lstat']**2\n",
    "BostonData['lstat_3'] = BostonData['lstat']**3\n",
    "BostonData['lstat_4'] = BostonData['lstat']**4\n",
    "BostonData['lstat_5'] = BostonData['lstat']**5\n",
    "X1 = BostonData[['lstat']]\n",
    "X2 = BostonData[['lstat','lstat_2']]\n",
    "X3 = BostonData[['lstat','lstat_2','lstat_3']]\n",
    "X4 = BostonData[['lstat','lstat_2','lstat_3','lstat_4']]\n",
    "X5 = BostonData[['lstat','lstat_2','lstat_3','lstat_4','lstat_5']]\n",
    "y = BostonData['medv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now divide your dataset into 25% test set and 75% training set and use Validation and MSE of test set to decide which degree of polynomial fits the best. Run this procedure a few times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x113271ad0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEPCAYAAAC9RFRvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nth9fXQjKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112daf350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "MSE_test = np.zeros(5)\n",
    "MSE_train = np.zeros(5)\n",
    "i = 0\n",
    "for X in [X1,X2,X3,X4,X5]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.25)\n",
    "    #We train based on Training Data BUT will Test on Test Sample\n",
    "    Model_train = lm.fit(X_train,y_train)\n",
    "    y_Hat_train = lm.predict(X_train)\n",
    "    y_Hat_test  = lm.predict(X_test)\n",
    "    MSE_test[i] = (metrics.mean_squared_error(lm.predict(X_test),y_test))\n",
    "    MSE_train[i] = (metrics.mean_squared_error(lm.predict(X_train),y_train))\n",
    "    i += 1\n",
    "\n",
    "index = np.array(range(5)) + 1\n",
    "MSE_Test_df = pd.DataFrame({'MSE_test':MSE_test,'index':index})\n",
    "MSE_Test_df.plot(x = 'index',y= 'MSE_test')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: A polynomial of degree 3 (X3) is the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, on the same data set, use 10 fold cross-validation to decide on the degree of polynomial. Justify what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.060144354446749, 30.948500278660553, 29.534539964263381, 28.306308279375571, 27.643394520014205]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1133cb810>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEPCAYAAAC9RFRvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113382bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = cross_validation.KFold(len(BostonData), n_folds = 10, shuffle = True)\n",
    "MSE_CV = []\n",
    "\n",
    "for i in [X1,X2,X3,X4,X5]:\n",
    "    scores = []\n",
    "    for train_index, test_index in kf:\n",
    "        lm = linear_model.LinearRegression().fit(i.iloc[train_index], y.iloc[train_index])\n",
    "        scores.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(i.iloc[test_index])))\n",
    "    MSE_CV.append(np.mean(scores))\n",
    "        \n",
    "        \n",
    "print(MSE_CV)\n",
    "index = np.array(range(5)) + 1\n",
    "MSE_CV_df = pd.DataFrame({'MSE_CV':MSE_CV,'index':index})\n",
    "MSE_CV_df.plot(x = 'index',y= 'MSE_CV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: According to this analysis, the most fitted model is X5 but using Hamed's advice we would pick either X2 or X3 because he recommends that, to avoid overfitting, it is better to keep the model simple.  Choosing X2 would be choosing the simplest model, but would leave the improvements that the additional features obviously add to the accuracy of the predicted values.  Choosing X3 takes advantage of the additional accuracy.  Besides the \"keep it simple\" advice, the change in accuracy between the X2 model and the X3 model is greater than the change in accuracy between X3 and X4.  If there had been a more significant increase in accuracy, then it might have been better to choose X4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's consider more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first focus on correlation Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim          zn       indus        chas         nox          rm  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              age         dis         rad         tax     ptratio       black  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            lstat        medv  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first get rid of additional variables we added to our dataframe\n",
    "del BostonData['lstat_2']\n",
    "del BostonData['lstat_3']\n",
    "del BostonData['lstat_4']\n",
    "del BostonData['lstat_5']\n",
    "BostonData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.388305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.360445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.483725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.175260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.427321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.695360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.376955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>-0.381626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>-0.468536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.507787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>0.333461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.737663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medv</th>\n",
       "      <td>-0.388305</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim        zn     indus      chas       nox        rm       age  \\\n",
       "crim     1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
       "zn      -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "indus    0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "chas    -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "nox      0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "rm      -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "age      0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "dis     -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "rad      0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "tax      0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "ptratio  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "black   -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "lstat    0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "medv    -0.388305  0.360445 -0.483725  0.175260 -0.427321  0.695360 -0.376955   \n",
       "\n",
       "              dis       rad       tax   ptratio     black     lstat      medv  \n",
       "crim    -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621 -0.388305  \n",
       "zn       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  0.360445  \n",
       "indus   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800 -0.483725  \n",
       "chas    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  0.175260  \n",
       "nox     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879 -0.427321  \n",
       "rm       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  0.695360  \n",
       "age     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339 -0.376955  \n",
       "dis      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  0.249929  \n",
       "rad     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676 -0.381626  \n",
       "tax     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993 -0.468536  \n",
       "ptratio -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044 -0.507787  \n",
       "black    0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  0.333461  \n",
       "lstat   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000 -0.737663  \n",
       "medv     0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  1.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BostonData.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List 3 variables that have the highest chance to appear in your final model - the model that can predict medv. Can these variables appear simultaneously in your final model if your goal is interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: lstat (% lower economic status), ptratio (pupil teacher ratio), rm (number of rooms per dwelling) -- lstat and rm  have a strong negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's standardize our data and put it in a new DataFrame called BostonDataNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419782</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "      <td>0.159686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "      <td>-0.101524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "      <td>1.324247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "      <td>1.182758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "      <td>1.487503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       crim        zn     indus      chas       nox        rm       age  \\\n",
       "0 -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
       "1 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
       "2 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
       "3 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
       "4 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
       "\n",
       "        dis       rad       tax   ptratio     black     lstat      medv  \n",
       "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562  0.159686  \n",
       "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439 -0.101524  \n",
       "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727  1.324247  \n",
       "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517  1.182758  \n",
       "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501  1.487503  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BostonDataNew = preprocessing.scale(BostonData) #CreditDataNew is now a numpy array\n",
    "BostonDataNew = pd.DataFrame(BostonDataNew)   #We changed CreditDataNew to a dataframe\n",
    "BostonDataNew.columns = BostonData.columns.values  #We renamed columns of CreditDataNew\n",
    "BostonDataNew.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use 10-fold cross validation and Lasso regression on our standardized data to decide which variables to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crim' 'zn' 'indus' 'chas' 'nox' 'rm' 'age' 'dis' 'rad' 'tax' 'ptratio'\n",
      " 'black' 'lstat' 'medv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.388305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.360445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.483725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.175260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.427321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.695360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.376955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>-0.381626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>-0.468536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.507787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>0.333461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.737663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medv</th>\n",
       "      <td>-0.388305</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim        zn     indus      chas       nox        rm       age  \\\n",
       "crim     1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
       "zn      -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "indus    0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "chas    -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "nox      0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "rm      -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "age      0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "dis     -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "rad      0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "tax      0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "ptratio  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "black   -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "lstat    0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "medv    -0.388305  0.360445 -0.483725  0.175260 -0.427321  0.695360 -0.376955   \n",
       "\n",
       "              dis       rad       tax   ptratio     black     lstat      medv  \n",
       "crim    -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621 -0.388305  \n",
       "zn       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  0.360445  \n",
       "indus   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800 -0.483725  \n",
       "chas    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  0.175260  \n",
       "nox     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879 -0.427321  \n",
       "rm       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  0.695360  \n",
       "age     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339 -0.376955  \n",
       "dis      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  0.249929  \n",
       "rad     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676 -0.381626  \n",
       "tax     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993 -0.468536  \n",
       "ptratio -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044 -0.507787  \n",
       "black    0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  0.333461  \n",
       "lstat   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000 -0.737663  \n",
       "medv     0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfAllVariables = BostonDataNew.columns.values\n",
    "print listOfAllVariables\n",
    "X = BostonDataNew[listOfAllVariables]\n",
    "del X['medv']\n",
    "#del X['lstat_2']\n",
    "y = BostonDataNew['medv']\n",
    "BostonDataNew.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1e-10 0.280498591628\n",
      "Alpha: 1e-09 0.280498591452\n",
      "Alpha: 1e-08 0.28049858967\n",
      "Alpha: 1e-07 0.280498572224\n",
      "Alpha: 1e-06 0.280498393079\n",
      "Alpha: 1e-05 0.280496656346\n",
      "Alpha: 0.0001 0.280479281011\n",
      "Alpha: 0.001 0.280306780343\n",
      "Alpha: 0.01 0.285180010312\n",
      "Alpha: 0.1 0.345812688552\n",
      "Alpha: 1.0 1.00239327639\n",
      "Alpha: 10.0 1.00239327639\n",
      "Alpha: 100.0 1.00239327639\n",
      "Alpha: 1000.0 1.00239327639\n",
      "Alpha: 10000.0 1.00239327639\n",
      "Alpha: 100000.0 1.00239327639\n",
      "Alpha: 1000000.0 1.00239327639\n",
      "Alpha: 10000000.0 1.00239327639\n",
      "Alpha: 100000000.0 1.00239327639\n",
      "Alpha: 1000000000.0 1.00239327639\n",
      "Alpha: 10000000000.0 1.00239327639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11339d450>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFO4iIgFSuIuIBOg/AREqbToYl7JcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137bfe50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = cross_validation.KFold(len(BostonDataNew), n_folds = 10, shuffle = True)\n",
    "MSE_Lasso_CV = []\n",
    "alphas = np.logspace(-10, 10, 21) #this is the lambda\n",
    "alphas_index = np.linspace(-10,10,21)\n",
    "scores = []\n",
    "for a in alphas:\n",
    "    \n",
    "    scores = []\n",
    "    for train_index, test_index in kf:\n",
    "        lm = linear_model.Lasso(alpha=a).fit(X.iloc[train_index], y.iloc[train_index])\n",
    "        scores.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(X.iloc[test_index])))\n",
    "    print 'Alpha:', a, np.mean(scores)\n",
    "    MSE_Lasso_CV.append(np.mean(scores))\n",
    "\n",
    "\n",
    "\n",
    "index = alphas\n",
    "MSE_Lasso_CV_df = pd.DataFrame({'MSE_Lasso_CV': MSE_Lasso_CV ,'Log_Alphas': alphas_index })\n",
    "MSE_Lasso_CV_df.plot(x = 'Log_Alphas',y = 'MSE_Lasso_CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.07146127512738705, 'crim'), (0.080283910755197474, 'zn'), (-0.0, 'indus'), (0.071857996341448266, 'chas'), (-0.17520463681735893, 'nox'), (0.3062111332903063, 'rm'), (-0.0, 'age'), (-0.26996579836071732, 'dis'), (0.14261427135427754, 'rad'), (-0.10216247651642626, 'tax'), (-0.21033999186068356, 'ptratio'), (0.083704310613461827, 'black'), (-0.40556056135991542, 'lstat')]\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.Lasso(alpha=10**(-2))\n",
    "lm.fit(X, y)\n",
    "print zip(lm.coef_,X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Choose rm or lstat due to high collinearity and also choose ptratio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use 10-fold cross validation to choose our best model among the following candidates. Let's first add lstat**2 to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BostonData['lstat_2'] = BostonData['lstat']**2\n",
    "X1 = BostonData[['lstat']]\n",
    "X2 = BostonData[['lstat','lstat_2']]\n",
    "X3 = BostonData[['lstat','chas']]\n",
    "X4 = BostonData[['lstat','lstat_2','chas']] #'black' is highly correlated with lstat so cannot consider them simoltanously\n",
    "X5 = BostonData[['ptratio','chas']]\n",
    "X6 = BostonData[['ptratio','chas','black']]\n",
    "X7 = BostonData[['ptratio','black']]\n",
    "X8 = BostonData[['rm']]\n",
    "X9 = BostonData[['rm','chas']]\n",
    "X10 = BostonData[['rm','chas','black']]\n",
    "X11 = BostonData[['rm','black']]\n",
    "X12 = BostonData[['lstat','ptratio','rm']]  #model without that much interpretability\n",
    "X13 = BostonData[['lstat','lstat_2','ptratio','rm']]  #model without that much interpretability\n",
    "X14 = BostonData[['lstat','ptratio','rm','chas','black']]  #model without that much interpretability\n",
    "X15 = BostonData[['lstat','lstat_2','ptratio','rm','chas','black']]  #model without that much interpretability\n",
    "y = BostonData['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_alpha</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.363915</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>0.348929</td>\n",
       "      <td>0.344991</td>\n",
       "      <td>0.686327</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.523180</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>0.463622</td>\n",
       "      <td>0.333655</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.325302</td>\n",
       "      <td>0.272728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.363915</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>0.348929</td>\n",
       "      <td>0.344991</td>\n",
       "      <td>0.686327</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.523180</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>0.463622</td>\n",
       "      <td>0.333655</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.325302</td>\n",
       "      <td>0.272728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.363915</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>0.348929</td>\n",
       "      <td>0.344991</td>\n",
       "      <td>0.686327</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.523180</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>0.463622</td>\n",
       "      <td>0.333655</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.325302</td>\n",
       "      <td>0.272728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.363915</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>0.348929</td>\n",
       "      <td>0.344991</td>\n",
       "      <td>0.686327</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.523180</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>0.463622</td>\n",
       "      <td>0.333655</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.325302</td>\n",
       "      <td>0.272728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.363915</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>0.348929</td>\n",
       "      <td>0.344991</td>\n",
       "      <td>0.686327</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.523180</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>0.463622</td>\n",
       "      <td>0.333655</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.325302</td>\n",
       "      <td>0.272728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.363915</td>\n",
       "      <td>0.348535</td>\n",
       "      <td>0.348930</td>\n",
       "      <td>0.344992</td>\n",
       "      <td>0.686328</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.523180</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>0.463622</td>\n",
       "      <td>0.333655</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.325302</td>\n",
       "      <td>0.272728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-4</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.363914</td>\n",
       "      <td>0.348536</td>\n",
       "      <td>0.348941</td>\n",
       "      <td>0.345008</td>\n",
       "      <td>0.686331</td>\n",
       "      <td>0.688439</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.517705</td>\n",
       "      <td>0.460062</td>\n",
       "      <td>0.463620</td>\n",
       "      <td>0.333652</td>\n",
       "      <td>0.281713</td>\n",
       "      <td>0.325299</td>\n",
       "      <td>0.272726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3</td>\n",
       "      <td>0.462079</td>\n",
       "      <td>0.363910</td>\n",
       "      <td>0.348544</td>\n",
       "      <td>0.349045</td>\n",
       "      <td>0.345161</td>\n",
       "      <td>0.686378</td>\n",
       "      <td>0.688437</td>\n",
       "      <td>0.523172</td>\n",
       "      <td>0.517691</td>\n",
       "      <td>0.460052</td>\n",
       "      <td>0.463610</td>\n",
       "      <td>0.333635</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.325302</td>\n",
       "      <td>0.272721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2</td>\n",
       "      <td>0.462069</td>\n",
       "      <td>0.363899</td>\n",
       "      <td>0.348819</td>\n",
       "      <td>0.350154</td>\n",
       "      <td>0.346781</td>\n",
       "      <td>0.688322</td>\n",
       "      <td>0.688440</td>\n",
       "      <td>0.523290</td>\n",
       "      <td>0.519119</td>\n",
       "      <td>0.461543</td>\n",
       "      <td>0.463692</td>\n",
       "      <td>0.333734</td>\n",
       "      <td>0.281845</td>\n",
       "      <td>0.327008</td>\n",
       "      <td>0.274370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.462151</td>\n",
       "      <td>0.366388</td>\n",
       "      <td>0.370910</td>\n",
       "      <td>0.366791</td>\n",
       "      <td>0.369408</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.542885</td>\n",
       "      <td>0.542885</td>\n",
       "      <td>0.483282</td>\n",
       "      <td>0.483282</td>\n",
       "      <td>0.361845</td>\n",
       "      <td>0.310703</td>\n",
       "      <td>0.354938</td>\n",
       "      <td>0.305551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.480715</td>\n",
       "      <td>0.613859</td>\n",
       "      <td>0.444901</td>\n",
       "      <td>0.405547</td>\n",
       "      <td>0.405286</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>0.890873</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.891834</td>\n",
       "      <td>0.891834</td>\n",
       "      <td>0.480715</td>\n",
       "      <td>0.613859</td>\n",
       "      <td>0.479372</td>\n",
       "      <td>0.606401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.624447</td>\n",
       "      <td>0.479859</td>\n",
       "      <td>0.549637</td>\n",
       "      <td>0.492198</td>\n",
       "      <td>0.904554</td>\n",
       "      <td>0.904554</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.904554</td>\n",
       "      <td>0.904554</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.624447</td>\n",
       "      <td>0.904554</td>\n",
       "      <td>0.621932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.797158</td>\n",
       "      <td>0.751284</td>\n",
       "      <td>0.550243</td>\n",
       "      <td>0.561417</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.797158</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.797158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.763366</td>\n",
       "      <td>0.836320</td>\n",
       "      <td>0.647055</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.834534</td>\n",
       "      <td>0.728563</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.000834</td>\n",
       "      <td>0.888523</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>0.895094</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "      <td>1.003535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    log_alpha         1         2         3         4         5         6  \\\n",
       "0         -10  0.462080  0.363915  0.348535  0.348929  0.344991  0.686327   \n",
       "1          -9  0.462080  0.363915  0.348535  0.348929  0.344991  0.686327   \n",
       "2          -8  0.462080  0.363915  0.348535  0.348929  0.344991  0.686327   \n",
       "3          -7  0.462080  0.363915  0.348535  0.348929  0.344991  0.686327   \n",
       "4          -6  0.462080  0.363915  0.348535  0.348929  0.344991  0.686327   \n",
       "5          -5  0.462080  0.363915  0.348535  0.348930  0.344992  0.686328   \n",
       "6          -4  0.462080  0.363914  0.348536  0.348941  0.345008  0.686331   \n",
       "7          -3  0.462079  0.363910  0.348544  0.349045  0.345161  0.686378   \n",
       "8          -2  0.462069  0.363899  0.348819  0.350154  0.346781  0.688322   \n",
       "9          -1  0.462151  0.366388  0.370910  0.366791  0.369408  0.690446   \n",
       "10          0  0.480715  0.613859  0.444901  0.405547  0.405286  0.890873   \n",
       "11          1  1.003535  0.624447  0.479859  0.549637  0.492198  0.904554   \n",
       "12          2  1.003535  0.797158  0.751284  0.550243  0.561417  1.003535   \n",
       "13          3  1.003535  1.003535  0.763366  0.836320  0.647055  1.003535   \n",
       "14          4  1.003535  1.003535  1.003535  0.834534  0.728563  1.003535   \n",
       "15          5  1.003535  1.003535  1.003535  1.000834  0.888523  1.003535   \n",
       "16          6  1.003535  1.003535  1.003535  1.003535  0.895094  1.003535   \n",
       "17          7  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "18          8  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "19          9  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "20         10  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "\n",
       "           7         8         9        10        11        12        13  \\\n",
       "0   0.688439  0.523180  0.517709  0.460065  0.463622  0.333655  0.281715   \n",
       "1   0.688439  0.523180  0.517709  0.460065  0.463622  0.333655  0.281715   \n",
       "2   0.688439  0.523180  0.517709  0.460065  0.463622  0.333655  0.281715   \n",
       "3   0.688439  0.523180  0.517709  0.460065  0.463622  0.333655  0.281715   \n",
       "4   0.688439  0.523180  0.517709  0.460065  0.463622  0.333655  0.281715   \n",
       "5   0.688439  0.523180  0.517709  0.460065  0.463622  0.333655  0.281715   \n",
       "6   0.688439  0.523179  0.517705  0.460062  0.463620  0.333652  0.281713   \n",
       "7   0.688437  0.523172  0.517691  0.460052  0.463610  0.333635  0.281700   \n",
       "8   0.688440  0.523290  0.519119  0.461543  0.463692  0.333734  0.281845   \n",
       "9   0.690446  0.542885  0.542885  0.483282  0.483282  0.361845  0.310703   \n",
       "10  0.890873  1.003535  1.003535  0.891834  0.891834  0.480715  0.613859   \n",
       "11  0.904554  1.003535  1.003535  0.904554  0.904554  1.003535  0.624447   \n",
       "12  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  0.797158   \n",
       "13  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "14  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "15  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "16  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "17  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "18  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "19  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "20  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535  1.003535   \n",
       "\n",
       "          14        15  \n",
       "0   0.325302  0.272728  \n",
       "1   0.325302  0.272728  \n",
       "2   0.325302  0.272728  \n",
       "3   0.325302  0.272728  \n",
       "4   0.325302  0.272728  \n",
       "5   0.325302  0.272728  \n",
       "6   0.325299  0.272726  \n",
       "7   0.325302  0.272721  \n",
       "8   0.327008  0.274370  \n",
       "9   0.354938  0.305551  \n",
       "10  0.479372  0.606401  \n",
       "11  0.904554  0.621932  \n",
       "12  1.003535  0.797158  \n",
       "13  1.003535  1.003535  \n",
       "14  1.003535  1.003535  \n",
       "15  1.003535  1.003535  \n",
       "16  1.003535  1.003535  \n",
       "17  1.003535  1.003535  \n",
       "18  1.003535  1.003535  \n",
       "19  1.003535  1.003535  \n",
       "20  1.003535  1.003535  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = cross_validation.KFold(len(BostonDataNew), n_folds = 10, shuffle = True)\n",
    "MSE_alpha_CV = []\n",
    "alphas = np.logspace(-10, 10, 21) #this is the lambda\n",
    "alphas_index = np.linspace(-10,10,21)\n",
    "scores = []\n",
    "MSE_alpha_CV_df = pd.DataFrame({'log_alpha': alphas_index })\n",
    "\n",
    "\n",
    "MSE_CV = []\n",
    "j=0\n",
    "\n",
    "for i in [X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15]:\n",
    "    for a in alphas:\n",
    "        scores = []\n",
    "        for train_index, test_index in kf:\n",
    "            lm = linear_model.Lasso(alpha=a).fit(i.iloc[train_index], y.iloc[train_index])\n",
    "            scores.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(i.iloc[test_index])))\n",
    "        #print 'Alpha:', a, np.mean(scores)\n",
    "        MSE_CV.append(np.mean(scores))\n",
    "    j=j+1\n",
    "    MSE_alpha_CV_df[j] = MSE_CV\n",
    "    MSE_CV = []\n",
    "    \n",
    "MSE_alpha_CV_df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [(-0.10143610622669484, 'lstat')] 0.322217491652\n",
      "\n",
      "2 [(-0.22748341478517356, 'lstat'), (0.0039701676005270657, 'lstat_2')] 0.234296040958\n",
      "\n",
      "3 [(-0.39056032352729453, 'lstat'), (0.014253998634844795, 'lstat_2'), (-0.00018318658092103537, 'lstat_3')] 0.21788701399\n",
      "\n",
      "4 [(-0.36424364572937978, 'lstat'), (0.012162366471613981, 'lstat_2'), (-0.00012515632369706955, 'lstat_3'), (-4.7154668846302353e-07, 'lstat_4')] 0.218955891784\n",
      "\n",
      "5 [(-0.38951799717734487, 'lstat'), (0.014463987336753874, 'lstat_2'), (-0.00015208147752642127, 'lstat_3'), (-3.4590964971854773e-06, 'lstat_4'), (7.0097582258700183e-08, 'lstat_5')] 0.217942696551\n",
      "\n",
      "6 [(-0.20826049189702944, 'ptratio'), (0.27159359041039577, 'chas'), (0.0027420842064602795, 'black')] 0.309565268736\n",
      "\n",
      "7 [(-0.21197415744066123, 'ptratio'), (0.002763327925522324, 'black')] 0.318590580879\n",
      "\n",
      "8 [(0.97035405817719877, 'rm')] 0.194469162878\n",
      "\n",
      "9 [(0.9606934322297993, 'rm'), (0.29288129720674383, 'chas')] 0.191492381561\n",
      "\n",
      "10 [(0.9166899616594375, 'rm'), (0.2563478257451417, 'chas'), (0.0027166664165553123, 'black')] 0.17733634395\n",
      "\n",
      "11 [(0.92469949857442568, 'rm'), (0.0027435675394173738, 'black')] 0.179786039944\n",
      "\n",
      "12 [(-0.063944763691279508, 'lstat'), (-0.10052700261413258, 'ptratio'), (0.46132128379769838, 'rm')] 0.228626619028\n",
      "\n",
      "13 [(-0.18051879499948967, 'lstat'), (0.0034625559214792388, 'lstat_2'), (-0.078911659436492851, 'ptratio'), (0.39214821408982892, 'rm')] 0.160403929633\n",
      "\n",
      "14 [(-0.057960332248744376, 'lstat'), (-0.09457408824160897, 'ptratio'), (0.48004190360967436, 'rm'), (0.20915860846435338, 'chas'), (0.0010959201387655128, 'black')] 0.214428885822\n",
      "\n",
      "15 [(-0.17369911472589564, 'lstat'), (0.0034150218139415464, 'lstat_2'), (-0.07342693439829015, 'ptratio'), (0.40845767962830903, 'rm'), (0.2270488408943335, 'chas'), (0.00096414212889656115, 'black')] 0.150102487226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j=1\n",
    "for i in [X1,X2]:\n",
    "    lm = linear_model.Lasso(alpha=10**(-1))\n",
    "    lm.fit(i, y)\n",
    "    print j, zip(lm.coef_,i.columns),metrics.mean_squared_error(y.iloc[test_index], lm.predict(i.iloc[test_index]))\n",
    "    j=j+1\n",
    "    print ''\n",
    "\n",
    "for i in [X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15]:\n",
    "    lm = linear_model.Lasso(alpha=10**(-2))\n",
    "    lm.fit(i, y)\n",
    "    print j, zip(lm.coef_,i.columns),metrics.mean_squared_error(y.iloc[test_index], lm.predict(i.iloc[test_index]))\n",
    "    j=j+1\n",
    "    print ''\n",
    "    \n",
    "# programming tasks:  how to get sorted list of mean square error with model number as index\n",
    "# how to create index with model name \"X1\" etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your goal is interpretation - what model(s) are you going to use? Use  smf.ols  in \"statsmodels.formula.api as smf\" to test significancy of your coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: don't understand what makes these more interpretable than the last set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.544\n",
      "Model:                            OLS   Adj. R-squared:                  0.543\n",
      "Method:                 Least Squares   F-statistic:                     601.6\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):           5.08e-88\n",
      "Time:                        12:11:30   Log-Likelihood:                -519.23\n",
      "No. Observations:                 506   AIC:                             1042.\n",
      "Df Residuals:                     504   BIC:                             1051.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.3083      0.061     21.366      0.000         1.188     1.429\n",
      "i             -0.1034      0.004    -24.528      0.000        -0.112    -0.095\n",
      "==============================================================================\n",
      "Omnibus:                      137.043   Durbin-Watson:                   0.892\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.373\n",
      "Skew:                           1.453   Prob(JB):                     5.36e-64\n",
      "Kurtosis:                       5.319   Cond. No.                         29.7\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.641\n",
      "Model:                            OLS   Adj. R-squared:                  0.639\n",
      "Method:                 Least Squares   F-statistic:                     448.5\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          1.56e-112\n",
      "Time:                        12:11:30   Log-Likelihood:                -459.00\n",
      "No. Observations:                 506   AIC:                             924.0\n",
      "Df Residuals:                     503   BIC:                             936.7\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.2126      0.095     23.311      0.000         2.026     2.399\n",
      "i[0]          -0.2539      0.013    -18.843      0.000        -0.280    -0.227\n",
      "i[1]           0.0047      0.000     11.628      0.000         0.004     0.006\n",
      "==============================================================================\n",
      "Omnibus:                      107.006   Durbin-Watson:                   0.921\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              228.388\n",
      "Skew:                           1.128   Prob(JB):                     2.55e-50\n",
      "Kurtosis:                       5.397   Cond. No.                     1.13e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.13e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.658\n",
      "Model:                            OLS   Adj. R-squared:                  0.656\n",
      "Method:                 Least Squares   F-statistic:                     321.7\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          1.78e-116\n",
      "Time:                        12:11:30   Log-Likelihood:                -446.64\n",
      "No. Observations:                 506   AIC:                             901.3\n",
      "Df Residuals:                     502   BIC:                             918.2\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.8425      0.156     18.203      0.000         2.536     3.149\n",
      "i[0]          -0.4207      0.036    -11.757      0.000        -0.491    -0.350\n",
      "i[1]           0.0162      0.002      6.983      0.000         0.012     0.021\n",
      "i[2]          -0.0002   4.35e-05     -5.013      0.000        -0.000    -0.000\n",
      "==============================================================================\n",
      "Omnibus:                      107.925   Durbin-Watson:                   0.906\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              258.171\n",
      "Skew:                           1.088   Prob(JB):                     8.69e-57\n",
      "Kurtosis:                       5.741   Cond. No.                     5.20e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.2e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.673\n",
      "Model:                            OLS   Adj. R-squared:                  0.670\n",
      "Method:                 Least Squares   F-statistic:                     257.8\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          4.16e-120\n",
      "Time:                        12:11:30   Log-Likelihood:                -435.17\n",
      "No. Observations:                 506   AIC:                             880.3\n",
      "Df Residuals:                     501   BIC:                             901.5\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      3.7851      0.248     15.252      0.000         3.297     4.273\n",
      "i[0]          -0.7650      0.080     -9.618      0.000        -0.921    -0.609\n",
      "i[1]           0.0539      0.008      6.616      0.000         0.038     0.070\n",
      "i[2]          -0.0018      0.000     -5.448      0.000        -0.002    -0.001\n",
      "i[3]        2.121e-05    4.4e-06      4.820      0.000      1.26e-05  2.99e-05\n",
      "==============================================================================\n",
      "Omnibus:                      128.764   Durbin-Watson:                   0.940\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              387.790\n",
      "Skew:                           1.197   Prob(JB):                     6.20e-85\n",
      "Kurtosis:                       6.558   Cond. No.                     2.59e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.59e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.682\n",
      "Model:                            OLS   Adj. R-squared:                  0.679\n",
      "Method:                 Least Squares   F-statistic:                     214.2\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          8.73e-122\n",
      "Time:                        12:11:30   Log-Likelihood:                -428.37\n",
      "No. Observations:                 506   AIC:                             868.7\n",
      "Df Residuals:                     500   BIC:                             894.1\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.9158      0.392     12.531      0.000         4.145     5.687\n",
      "i[0]          -1.3051      0.166     -7.859      0.000        -1.631    -0.979\n",
      "i[1]           0.1385      0.024      5.703      0.000         0.091     0.186\n",
      "i[2]          -0.0074      0.002     -4.747      0.000        -0.011    -0.004\n",
      "i[3]           0.0002   4.53e-05      4.143      0.000      9.88e-05     0.000\n",
      "i[4]       -1.776e-06   4.81e-07     -3.692      0.000     -2.72e-06 -8.31e-07\n",
      "==============================================================================\n",
      "Omnibus:                      144.085   Durbin-Watson:                   0.987\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              494.545\n",
      "Skew:                           1.292   Prob(JB):                    4.08e-108\n",
      "Kurtosis:                       7.096   Cond. No.                     1.37e+08\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.37e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.331\n",
      "Model:                            OLS   Adj. R-squared:                  0.327\n",
      "Method:                 Least Squares   F-statistic:                     82.63\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):           1.84e-43\n",
      "Time:                        12:11:30   Log-Likelihood:                -616.44\n",
      "No. Observations:                 506   AIC:                             1241.\n",
      "Df Residuals:                     502   BIC:                             1258.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.8445      0.375      7.582      0.000         2.107     3.582\n",
      "i[0]          -0.2083      0.017    -12.063      0.000        -0.242    -0.174\n",
      "i[1]           0.4272      0.145      2.945      0.003         0.142     0.712\n",
      "i[2]           0.0027      0.000      6.688      0.000         0.002     0.004\n",
      "==============================================================================\n",
      "Omnibus:                       92.909   Durbin-Watson:                   0.795\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              191.950\n",
      "Skew:                           1.000   Prob(JB):                     2.08e-42\n",
      "Kurtosis:                       5.260   Cond. No.                     3.79e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.79e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.319\n",
      "Model:                            OLS   Adj. R-squared:                  0.316\n",
      "Method:                 Least Squares   F-statistic:                     117.8\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):           1.08e-42\n",
      "Time:                        12:11:30   Log-Likelihood:                -620.78\n",
      "No. Observations:                 506   AIC:                             1248.\n",
      "Df Residuals:                     503   BIC:                             1260.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.9699      0.376      7.907      0.000         2.232     3.708\n",
      "i[0]          -0.2142      0.017    -12.389      0.000        -0.248    -0.180\n",
      "i[1]           0.0028      0.000      6.721      0.000         0.002     0.004\n",
      "==============================================================================\n",
      "Omnibus:                      108.924   Durbin-Watson:                   0.762\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              249.419\n",
      "Skew:                           1.116   Prob(JB):                     6.91e-55\n",
      "Kurtosis:                       5.618   Cond. No.                     3.77e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.77e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.484\n",
      "Model:                            OLS   Adj. R-squared:                  0.483\n",
      "Method:                 Least Squares   F-statistic:                     471.8\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):           2.49e-74\n",
      "Time:                        12:11:30   Log-Likelihood:                -550.82\n",
      "No. Observations:                 506   AIC:                             1106.\n",
      "Df Residuals:                     504   BIC:                             1114.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -6.2259      0.288    -21.588      0.000        -6.792    -5.659\n",
      "i              0.9907      0.046     21.722      0.000         0.901     1.080\n",
      "==============================================================================\n",
      "Omnibus:                      102.585   Durbin-Watson:                   0.684\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              612.449\n",
      "Skew:                           0.726   Prob(JB):                    1.02e-133\n",
      "Kurtosis:                       8.190   Cond. No.                         58.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.496\n",
      "Model:                            OLS   Adj. R-squared:                  0.494\n",
      "Method:                 Least Squares   F-statistic:                     247.6\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):           1.36e-75\n",
      "Time:                        12:11:30   Log-Likelihood:                -544.57\n",
      "No. Observations:                 506   AIC:                             1095.\n",
      "Df Residuals:                     503   BIC:                             1108.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -6.1645      0.286    -21.580      0.000        -6.726    -5.603\n",
      "i[0]           0.9760      0.045     21.555      0.000         0.887     1.065\n",
      "i[1]           0.4443      0.125      3.547      0.000         0.198     0.690\n",
      "==============================================================================\n",
      "Omnibus:                       92.470   Durbin-Watson:                   0.753\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              572.587\n",
      "Skew:                           0.619   Prob(JB):                    4.62e-125\n",
      "Kurtosis:                       8.062   Cond. No.                         58.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.555\n",
      "Model:                            OLS   Adj. R-squared:                  0.552\n",
      "Method:                 Least Squares   F-statistic:                     208.6\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):           7.81e-88\n",
      "Time:                        12:11:30   Log-Likelihood:                -513.20\n",
      "No. Observations:                 506   AIC:                             1034.\n",
      "Df Residuals:                     502   BIC:                             1051.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -6.8455      0.281    -24.320      0.000        -7.399    -6.292\n",
      "i[0]           0.9326      0.043     21.721      0.000         0.848     1.017\n",
      "i[1]           0.4083      0.118      3.462      0.001         0.177     0.640\n",
      "i[2]           0.0027      0.000      8.140      0.000         0.002     0.003\n",
      "==============================================================================\n",
      "Omnibus:                      145.855   Durbin-Watson:                   0.807\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              984.611\n",
      "Skew:                           1.073   Prob(JB):                    1.56e-214\n",
      "Kurtosis:                       9.488   Cond. No.                     3.52e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.52e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.544\n",
      "Model:                            OLS   Adj. R-squared:                  0.542\n",
      "Method:                 Least Squares   F-statistic:                     300.3\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):           1.47e-86\n",
      "Time:                        12:11:30   Log-Likelihood:                -519.17\n",
      "No. Observations:                 506   AIC:                             1044.\n",
      "Df Residuals:                     503   BIC:                             1057.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -6.9127      0.284    -24.352      0.000        -7.470    -6.355\n",
      "i[0]           0.9453      0.043     21.862      0.000         0.860     1.030\n",
      "i[1]           0.0027      0.000      8.187      0.000         0.002     0.003\n",
      "==============================================================================\n",
      "Omnibus:                      158.364   Durbin-Watson:                   0.739\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1053.751\n",
      "Skew:                           1.190   Prob(JB):                    1.52e-229\n",
      "Kurtosis:                       9.657   Cond. No.                     3.51e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.51e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "for i in [X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11]:\n",
    "    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n",
    "    print(lm1.summary())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your goal is prediction - what model(s) are you going to use? Use  smf.ols  in \"statsmodels.formula.api as smf\" to test significancy of your coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: model 1 : lstat, model 8 or 9: rm & chas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.679\n",
      "Model:                            OLS   Adj. R-squared:                  0.677\n",
      "Method:                 Least Squares   F-statistic:                     353.3\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          2.69e-123\n",
      "Time:                        10:44:06   Log-Likelihood:                -430.79\n",
      "No. Observations:                 506   AIC:                             869.6\n",
      "Df Residuals:                     502   BIC:                             886.5\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.4316      0.426     -1.013      0.311        -1.268     0.405\n",
      "i[0]          -0.0622      0.005    -13.540      0.000        -0.071    -0.053\n",
      "i[1]          -0.1013      0.013     -7.911      0.000        -0.126    -0.076\n",
      "i[2]           0.4914      0.046     10.603      0.000         0.400     0.583\n",
      "==============================================================================\n",
      "Omnibus:                      202.072   Durbin-Watson:                   0.901\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1022.153\n",
      "Skew:                           1.700   Prob(JB):                    1.10e-222\n",
      "Kurtosis:                       9.076   Cond. No.                         402.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.727\n",
      "Model:                            OLS   Adj. R-squared:                  0.725\n",
      "Method:                 Least Squares   F-statistic:                     333.3\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          1.19e-139\n",
      "Time:                        10:44:06   Log-Likelihood:                -389.65\n",
      "No. Observations:                 506   AIC:                             789.3\n",
      "Df Residuals:                     501   BIC:                             810.4\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.3540      0.402      0.881      0.379        -0.435     1.143\n",
      "i[0]          -0.1796      0.013    -13.629      0.000        -0.205    -0.154\n",
      "i[1]           0.0035      0.000      9.406      0.000         0.003     0.004\n",
      "i[2]          -0.0795      0.012     -6.606      0.000        -0.103    -0.056\n",
      "i[3]           0.4218      0.043      9.717      0.000         0.337     0.507\n",
      "==============================================================================\n",
      "Omnibus:                      175.209   Durbin-Watson:                   0.897\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              884.420\n",
      "Skew:                           1.441   Prob(JB):                    8.93e-193\n",
      "Kurtosis:                       8.800   Cond. No.                     5.48e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.48e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.696\n",
      "Model:                            OLS   Adj. R-squared:                  0.693\n",
      "Method:                 Least Squares   F-statistic:                     228.9\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          9.17e-127\n",
      "Time:                        10:44:06   Log-Likelihood:                -416.73\n",
      "No. Observations:                 506   AIC:                             845.5\n",
      "Df Residuals:                     500   BIC:                             870.8\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -1.1623      0.454     -2.562      0.011        -2.054    -0.271\n",
      "i[0]          -0.0564      0.005    -11.792      0.000        -0.066    -0.047\n",
      "i[1]          -0.0934      0.013     -7.432      0.000        -0.118    -0.069\n",
      "i[2]           0.5063      0.046     11.076      0.000         0.417     0.596\n",
      "i[3]           0.3613      0.098      3.683      0.000         0.169     0.554\n",
      "i[4]           0.0011      0.000      3.743      0.000         0.001     0.002\n",
      "==============================================================================\n",
      "Omnibus:                      205.836   Durbin-Watson:                   0.978\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1116.216\n",
      "Skew:                           1.709   Prob(JB):                    4.14e-243\n",
      "Kurtosis:                       9.423   Cond. No.                     6.81e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.81e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.743\n",
      "Model:                            OLS   Adj. R-squared:                  0.740\n",
      "Method:                 Least Squares   F-statistic:                     240.5\n",
      "Date:                Mon, 22 Feb 2016   Prob (F-statistic):          9.64e-144\n",
      "Time:                        10:44:06   Log-Likelihood:                -374.15\n",
      "No. Observations:                 506   AIC:                             762.3\n",
      "Df Residuals:                     499   BIC:                             791.9\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.3042      0.427     -0.712      0.477        -1.143     0.535\n",
      "i[0]          -0.1733      0.013    -13.339      0.000        -0.199    -0.148\n",
      "i[1]           0.0034      0.000      9.564      0.000         0.003     0.004\n",
      "i[2]          -0.0721      0.012     -6.117      0.000        -0.095    -0.049\n",
      "i[3]           0.4340      0.043     10.154      0.000         0.350     0.518\n",
      "i[4]           0.3794      0.090      4.201      0.000         0.202     0.557\n",
      "i[5]           0.0010      0.000      3.571      0.000         0.000     0.001\n",
      "==============================================================================\n",
      "Omnibus:                      175.562   Durbin-Watson:                   0.977\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              958.482\n",
      "Skew:                           1.416   Prob(JB):                    7.38e-209\n",
      "Kurtosis:                       9.119   Cond. No.                     8.20e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.2e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "for i in [X12,X13,X14,X15]:\n",
    "    lm1 = smf.ols(formula='y ~ i', data=BostonDataNew).fit()\n",
    "    print(lm1.summary())\n",
    "    \n",
    "    #All of our models are highly significant, so we use model 15. It generates the least CV-MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
