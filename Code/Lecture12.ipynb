{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are going to use Boston Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim  zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/ga-students/SF-DAT-20/master/Data/Boston.csv\"\n",
    "BostonData = pd.read_csv(url)\n",
    "del BostonData['Unnamed: 0']\n",
    "BostonData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = BostonData[BostonData.columns.values]\n",
    "del X['medv']\n",
    "y = BostonData['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For documentation of RandomForestRegressor please refer to:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=4, max_leaf_nodes=None, min_samples_leaf=5,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10000, n_jobs=1, oob_score=True, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestRegressor(n_estimators = 10000, #Number of trees - the more the better!\n",
    "                           max_features = 4,     #How many features to randomly choose in each node \n",
    "                           min_samples_leaf = 5, #Minimum number of observations at each terminal node\n",
    "                           oob_score = True)      #If you would like to record out of bage scores\n",
    "# if you do not set maximum number of features, then you'll have bagging\n",
    "\n",
    "RF.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0030280648264575344, 'chas'),\n",
       " (0.0061788379013354424, 'zn'),\n",
       " (0.0069839135030690062, 'rad'),\n",
       " (0.01385244605003752, 'black'),\n",
       " (0.022437173702383063, 'age'),\n",
       " (0.030540419856397993, 'tax'),\n",
       " (0.042334375328185764, 'dis'),\n",
       " (0.052550984317263814, 'crim'),\n",
       " (0.065821298188557337, 'indus'),\n",
       " (0.066696648942732834, 'nox'),\n",
       " (0.068292836057306641, 'ptratio'),\n",
       " (0.30807116755650632, 'lstat'),\n",
       " (0.3132118337697673, 'rm')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(RF.feature_importances_,X.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84694371284976977"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.oob_score_  #this value is like R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A time-consuming way to tune your model on m - # of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "Features = range(1,14)\n",
    "score = []\n",
    "for i in Features:\n",
    "      RF = RandomForestRegressor(n_estimators = 1000, #Number of trees - the more the better!\n",
    "                           max_features = i,     #How many features to randomly choose in each node \n",
    "                           min_samples_leaf = 5, #Minimum number of observations at each terminal node\n",
    "                           oob_score = True) \n",
    "      scores = cross_val_score(RF, X, y, cv=10, scoring='mean_squared_error')\n",
    "      score.append(np.mean(np.sqrt(-scores)))\n",
    "\n",
    "Depth_Choice_df = pd.DataFrame({'cv_scores': score ,'Number of Features': Features})\n",
    "Depth_Choice_df.plot(x ='Number of Features',y = 'cv_scores' )\n",
    "\n",
    "\n",
    "\n",
    "# Remember in our previous lecture, we could only achieve error of 5.2. Here we reduced it to 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check if choosing 1000 for number of trees was a good choice. (This may take a few minutes to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "NumberOfTrees = [1000,2000,5000,10000]\n",
    "score = []\n",
    "for i in NumberOfTrees:\n",
    "      RF = RandomForestRegressor(n_estimators = i, #Number of trees - the more the better!\n",
    "                           max_features = 7,     #How many features to randomly choose in each node \n",
    "                           min_samples_leaf = 5, #Minimum number of observations at each terminal node\n",
    "                           oob_score = True) \n",
    "      scores = cross_val_score(RF, X, y, cv=10, scoring='mean_squared_error')\n",
    "      score.append(np.mean(np.sqrt(-scores)))\n",
    "\n",
    "Depth_Choice_df = pd.DataFrame({'cv_scores': score ,'Number of Trees': NumberOfTrees })\n",
    "Depth_Choice_df.plot(x ='Number of Trees',y = 'cv_scores' )\n",
    "#look at the magnitude of CV_error - it is so small. It sounds like we don't see that much improvement after 1000 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Out of Bag error to tune our models. This is so much faster than CV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "Features = range(1,14)\n",
    "oob_score_RF = []\n",
    "for i in Features:\n",
    "      RF = RandomForestRegressor(n_estimators = 1000, #Number of trees - the more the better!\n",
    "                           max_features = i,     #How many features to randomly choose in each node \n",
    "                           min_samples_leaf = 5, #Minimum number of observations at each terminal node\n",
    "                           oob_score = True)\n",
    "      RF.fit(X,y)  \n",
    "      oob_score_RF.append(RF.oob_score_)\n",
    "\n",
    "Depth_Choice_df = pd.DataFrame({'Out of Bag Score': oob_score_RF ,'Number of Features': Features})\n",
    "Depth_Choice_df.plot(x ='Number of Features',y = 'Out of Bag Score' )\n",
    "\n",
    "# Out of bag score is R^2 - the larger the better. Again we can see our model works best when Number of features\n",
    "# is around 6. Almost same story as CV - but in much faster time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's first categorize medv to 4 groups. \n",
    "Lowest 20% medv is categorized as Level 1\n",
    "\n",
    "next 30% medv is categorized as Level 2\n",
    "\n",
    "next 30% medv is categorized as Level 3\n",
    "\n",
    "Top 20% medv is categorized as Level 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BostonData['medvCategory'] = '1'\n",
    "mask_cat2 = ((BostonData['medv'] > BostonData['medv'].quantile(.2)) & \n",
    "            (BostonData['medv'] <= BostonData['medv'].quantile(.5)))   \n",
    "mask_cat3 = ((BostonData['medv'] > BostonData['medv'].quantile(.5)) & \n",
    "            (BostonData['medv'] <= BostonData['medv'].quantile(.8)))  \n",
    "mask_cat4 = (BostonData['medv'] > BostonData['medv'].quantile(.8)) \n",
    "\n",
    "BostonData.loc[mask_cat2,'medvCategory'] = '2'\n",
    "BostonData.loc[mask_cat3,'medvCategory'] = '3'\n",
    "BostonData.loc[mask_cat4,'medvCategory'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = BostonData['medvCategory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for RandomForestClassifier http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFClass = RandomForestClassifier(n_estimators = 1000, \n",
    "                                 max_features = 4, # You can set it to a number or 'sqrt', 'log2', etc\n",
    "                                 min_samples_leaf = 5,\n",
    "                                 oob_score = True)\n",
    "RFClass.fit(X,y)\n",
    "print(RFClass.oob_score_)\n",
    "scores = cross_val_score(RFClass, X, y, cv=10)\n",
    "print(scores.mean())\n",
    "#out of bag error = 25% , CV_error is 35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "Features = range(1,14)\n",
    "oob_score_RF = []\n",
    "for i in Features:\n",
    "        RFClass = RandomForestClassifier(n_estimators = 1000, #Number of trees - the more the better!\n",
    "                           max_features = i,     #How many features to randomly choose in each node \n",
    "                           min_samples_leaf = 5, #Minimum number of observations at each terminal node\n",
    "                           oob_score = True)\n",
    "        RFClass.fit(X,y)  \n",
    "        oob_score_RF.append(RFClass.oob_score_)\n",
    "\n",
    "Depth_Choice_df = pd.DataFrame({'Out of Bag Score': oob_score_RF ,'Number of Features': Features})\n",
    "Depth_Choice_df.plot(x ='Number of Features',y = 'Out of Bag Score' )\n",
    "\n",
    "# it sounds like choosing max_features as 5 is our best bet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = RFClass.predict(X)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithm - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full documention, please refer to: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = BostonData['medv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBR_Tree = GradientBoostingRegressor(learning_rate = 0.01, # This is lambda, a tuning parameter, usually between 0.01 and 0.1\n",
    "                                     n_estimators = 10000, #This is B, a tuning parameter, using large B can cause overfitting\n",
    "                                     max_depth = 2, #This is d, another tuning parameter, usually max_depth < 5\n",
    "                                     min_samples_leaf = 5  )\n",
    "scores = cross_val_score(GBR_Tree, X, y, cv=10, scoring='mean_squared_error')\n",
    "print (np.mean(np.sqrt(-scores))) #it gave us an error of 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning 3 parameters simultanously is a very time-consuming task. Let's tune it step by step and via a semi-greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Depth = range(1,5)\n",
    "Score = []\n",
    "for i in Depth:\n",
    "                GBR_Tree = GradientBoostingRegressor(learning_rate = 0.01, # This is lambda, a tuning parameter, usually between 0.01 and 0.1\n",
    "                                             n_estimators = 10000, #This is B, a tuning parameter, using large B can cause overfitting\n",
    "                                             max_depth = i, #This is d, another tuning parameter, usually max_depth < 5\n",
    "                                             min_samples_leaf = 5  )  \n",
    "                scores = cross_val_score(GBR_Tree, X, y, cv=10, scoring='mean_squared_error')\n",
    "                Score.append(np.mean(np.sqrt(-scores)))\n",
    "\n",
    "Depth_Choice_df = pd.DataFrame({'CV_Error': Score ,'Max_Depth': Depth})\n",
    "Depth_Choice_df.plot(x ='Max_Depth',y = 'CV_Error' )\n",
    "\n",
    "# it seems like Max_Depth = 2 gives us the lowest CV-Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Score = []\n",
    "NumberOfTrees = [100,1000,5000,10000,20000]\n",
    "for i in NumberOfTrees:\n",
    "                GBR_Tree = GradientBoostingRegressor(learning_rate = 0.01, # This is lambda, a tuning parameter, usually between 0.01 and 0.1\n",
    "                                             n_estimators = i, #This is B, a tuning parameter, using large B can cause overfitting\n",
    "                                             max_depth = 2, #This is d, another tuning parameter, usually max_depth < 5\n",
    "                                             min_samples_leaf = 5  )  \n",
    "                scores = cross_val_score(GBR_Tree, X, y, cv=10, scoring='mean_squared_error')\n",
    "                Score.append(np.mean(np.sqrt(-scores)))\n",
    "\n",
    "Depth_Choice_df = pd.DataFrame({'CV_Error': Score ,'Number Of Trees': NumberOfTrees})\n",
    "Depth_Choice_df.plot(x ='Number Of Trees',y = 'CV_Error' )\n",
    "\n",
    "# it seems like at Tree_Size = 1000 we have reduced our error to 3.8 which is great! \n",
    "# You can localize your search. Even further. For example try size of tree from 500 to 5000 with increments of 100\n",
    "# Remember Learning Rate and Number of trees should be tuned with one another. Lower leawrning rates require more trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithm - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the following link for its documentation: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = BostonData['medvCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC_Tree = GradientBoostingClassifier(learning_rate = 0.01,\n",
    "                                      n_estimators = 1000,\n",
    "                                      max_depth = 2,\n",
    "                                      min_samples_leaf = 5)\n",
    "\n",
    "\n",
    "# Unfortunately we cannot use cross_val_score in this case. cross_val_score returns log of loss which is not a \n",
    "# measure we are interested in, in this course. \n",
    "\n",
    "from sklearn import cross_validation\n",
    "kf = cross_validation.KFold(len(BostonData), n_folds = 10, shuffle = True) #10 fold CV\n",
    "scores = []\n",
    "for train_index, test_index in kf:        \n",
    "        GBC_Tree.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "        y_hat_test = GBC_Tree.predict(X.iloc[test_index]) \n",
    "        scores.append(float(sum(y_hat_test == y.iloc[test_index]))/len(y_hat_test))\n",
    "\n",
    "Score_GBC_CV = np.mean(scores)    \n",
    "\n",
    "print(Score_GBC_CV) #Based on this setting, 73.7% of time we predict correct outcomes\n",
    "# in order to improve this result we need to tune our algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune your algorithm to get better predictibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
